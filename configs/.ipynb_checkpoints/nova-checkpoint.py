import json
import boto3
import botocore
from PIL import Image
import os
import base64
import imghdr
import io
from random import randint
#from datetime import datetime
import datetime
from IPython.display import display
from dateutil.tz import tzlocal
import time
import requests
from botocore.config import Config
from typing import Optional

from langchain import hub
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import FlashrankRerank
from langchain_aws.embeddings.bedrock import BedrockEmbeddings
from langchain.prompts.chat import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.document_loaders.merge import MergedDataLoader
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders.pdf import PyMuPDFLoader
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
from langchain_aws import ChatBedrockConverse
from botocore.exceptions import ClientError

from openai import OpenAI

retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")

region = 'us-east-1'
# Call Bedrock
bedrock_runtime = boto3.client(
    "bedrock-runtime",
    region_name=region,  # You must use us-east-1 during the beta period.
    endpoint_url=f"https://bedrock-runtime.{region}.amazonaws.com",
    config=Config(read_timeout=350) 
)

s3 = boto3.client("s3",region_name=region)

###
# Cross region
###
def get_boto_client(
    assumed_role: Optional[str] = None,
    region: Optional[str] = None,
    runtime: Optional[bool] = True,
    service_name: Optional[str] = None,
):
    """Create a boto3 client for Amazon Bedrock, with optional configuration overrides

    Parameters
    ----------
    assumed_role :
        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not
        specified, the current active credentials will be used.
    region :
        Optional name of the AWS Region in which the service should be called (e.g. "us-east-1").
        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.
    runtime :
        Optional choice of getting different client to perform operations with the Amazon Bedrock service.
    """
    if region is None:
        target_region = os.environ.get("AWS_REGION", os.environ.get("AWS_DEFAULT_REGION"))
    else:
        target_region = region

    print(f"Create new client\n  Using region: {target_region}")
    session_kwargs = {"region_name": target_region}
    client_kwargs = {**session_kwargs}

    profile_name = os.environ.get("AWS_PROFILE")
    if profile_name:
        print(f"  Using profile: {profile_name}")
        session_kwargs["profile_name"] = profile_name

    retry_config = Config(
        region_name=target_region,
        retries={
            "max_attempts": 10,
            "mode": "standard",
        },
    )
    session = boto3.Session(**session_kwargs)
    
    if assumed_role:
        sts = session.client("sts")
        response = sts.assume_role(
            RoleArn=str(assumed_role),
            RoleSessionName="langchain-llm-1"
        )
        print(" ... successful!")
        client_kwargs["aws_access_key_id"] = response["Credentials"]["AccessKeyId"]
        client_kwargs["aws_secret_access_key"] = response["Credentials"]["SecretAccessKey"]
        client_kwargs["aws_session_token"] = response["Credentials"]["SessionToken"]

    if not service_name:
        if runtime:
            service_name='bedrock-runtime'
        else:
            service_name='bedrock'

    bedrock_client = session.client(
        service_name=service_name,
        config=retry_config,
        **client_kwargs
    )

    return bedrock_client

###
# Urls checking
###
def check_urls(urls: list, kind: str):
    pdf_urls = []
    for url in urls:
        try:
            response = requests.head(url)
            if kind in response.url.lower():
                pdf_urls.append(url)
        except requests.exceptions.RequestException:
            pass
    return pdf_urls

## 
# Image gen utility
##
def nova_generate_image(model_id, body):
    """
    Generate an image using Amazon Nova Canvas model on demand.
    Args:
    model_id (str): The model ID to use.
    body (str) : The request body to use. 
    Returns:
    image_bytes (bytes): The image generated by the model.
    """
    logger.info("Generating image with Amazon Nova Canvas model %s", model_id)

    accept = "application/json"
    content_type = "application/json"
    response = bedrock.invoke_model(
        body=body, modelId=model_id, accept=accept, contentType=content_type
    )
    response_body = json.loads(response.get("body").read())
    base64_image = response_body.get("images")[0]
    base64_bytes = base64_image.encode('ascii')
    image_bytes = base64.b64decode(base64_bytes)
    finish_reason = response_body.get("error")
    
    if finish_reason is not None:
        raise ImageError(f"Image generation error. Error is {finish_reason}")
        
    logger.info(
        "Successfully generated image with Amazon Nova Canvas model %s", model_id
    )
    return image_bytes

###
# Text-to-image
###
def t2i_canvas(prompt:str, model_id:str, neg_prompt:str, cfgScale:float=7.0, num_image: int=1, width:int=1280, height:int=768, quality:str='premium', region_name:str='us-east-1'):

    # Configure the inference parameters.
    inference_params = {
        "taskType": "TEXT_IMAGE",
        "textToImageParams": {
            "text": prompt,
            "negativeText": neg_prompt  # A list of items that should not appear in the image.
        },
        "imageGenerationConfig": {
            "numberOfImages": num_image,  # Number of images to generate, up to 5.
            "width": width,  # See README for supported resolutions.
            "height": height,  # See README for supported resolutions.
            "cfgScale": cfgScale,  # How closely the prompt will be followed.
            "quality": quality,  # "standard" or "premium"
            "seed": randint(
                0, 2147483646
            ),  # Using a random seed value guarantees we get different results each time this code is executed.
        },
    }
    accept = "application/json"
    content_type = "application/json" 

    # Display the random seed.
    print(f"Generating with seed: {inference_params['imageGenerationConfig']['seed']}")
    
    start_time = datetime.datetime.now()

    # Invoke the model.
    #bedrock_runtime = get_boto_client(service_name='bedrock-runtime', region=region_name)
    response = bedrock_runtime.invoke_model(
        modelId=model_id,
        body=json.dumps(inference_params),
        accept=accept,
        contentType=content_type 
    )
    # Convert the JSON-formatted response to a dictionary.
    response_body = json.loads(response["body"].read())
    return_images = []
    try:
        images = response_body["images"]
        for i in range(len(images)):
            image_data = images[i]
            image_bytes = base64.b64decode(image_data)
            #image = Image.open(io.BytesIO(image_bytes))
            return_images.append(Image.open(io.BytesIO(image_bytes)))
    except:
        pass
    return return_images

###
# Image-to-image
###
def i2i_canvas(prompt:str, input_image, model_id:str, neg_prompt:str, cfgScale:float=7.0, num_image: int=1, width:int=1280, height:int=768, quality: str='premium', region_name: str='us=east-1'):
    # Move the cursor to the beginning of the BytesIO object
    input_image.seek(0)
    
    # Read the contents of the BytesIO object
    byte_data = input_image.read()
    
    # Encode the byte data to base64
    base64_encoded_data = base64.b64encode(byte_data).decode('utf8') 

    # Configure the inference parameters.
    inference_params = {
        "taskType": "IMAGE_VARIATION",
        "imageVariationParams": {
            "text": prompt,
            "negativeText": neg_prompt,
            "images": [base64_encoded_data],
            "similarityStrength": 0.7, # Range: 0.2 to 1.0
        },
        "imageGenerationConfig": {
            "numberOfImages": num_image,  # Number of images to generate, up to 5.
            "width": width,  # See README for supported resolutions.
            "height": height,  # See README for supported resolutions.
            "cfgScale": cfgScale,  # How closely the prompt will be followed.
            "quality": quality,  # "standard" or "premium"
            "seed": randint(
                0, 2147483646
            ),  # Using a random seed value guarantees we get different results each time this code is executed.
        },
    }
    accept = "application/json"
    content_type = "application/json" 

    # Display the random seed.
    print(f"Generating with seed: {inference_params['imageGenerationConfig']['seed']}")
    
    start_time = datetime.datetime.now()

    # Invoke the model.
    #bedrock_runtime = get_boto_client(service_name='bedrock-runtime', region=region_name)
    response = bedrock_runtime.invoke_model(
        modelId=model_id,
        body=json.dumps(inference_params),
        accept=accept,
        contentType=content_type 
    )
    # Convert the JSON-formatted response to a dictionary.
    response_body = json.loads(response["body"].read())
    return_images = []
    try:
        images = response_body["images"]
        for i in range(len(images)):
            image_data = images[i]
            image_bytes = base64.b64decode(image_data)
            #image = Image.open(io.BytesIO(image_bytes))
            return_images.append(Image.open(io.BytesIO(image_bytes)))
    except:
        pass
    return return_images


###
# Download video from S3
###
def download_video_for_invocation_arn(invocation_arn: str, role_arn:str, bucket_name: str, destination_folder:str="./output"):
    """
    This function downloads the video file for the given invocation ARN.
    """
    invocation_id = invocation_arn.split("/")[-1]

    # Create the local file path
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    file_name = f"{timestamp}_{invocation_id}.mp4"
    import os

    output_folder = os.path.abspath(destination_folder)
    local_file_path = os.path.join(output_folder, file_name)

    # Ensure the output folder exists
    os.makedirs(output_folder, exist_ok=True)
    '''
    # Create an STS client
    sts_client = boto3.client('sts')

    # Assume the specified role
    assumed_role = sts_client.assume_role(
        RoleArn=role_arn,
        RoleSessionName='CallBedrockOVG'
    )
    # Extract temporary credentials
    credentials = assumed_role['Credentials']
    
    # Create an S3 client
    s3 = boto3.client("s3",
                        aws_access_key_id=credentials['AccessKeyId'],
                        aws_secret_access_key=credentials['SecretAccessKey'],
                        aws_session_token=credentials['SessionToken'],
                     )
    '''
    
    # List objects in the specified folder
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=invocation_id)
    
    # Find the first MP4 file and download it.
    for obj in response.get("Contents", []):
        object_key = obj["Key"]
        if object_key.endswith(".mp4"):
            print(f"""Downloading "{object_key}"...""")
            s3.download_file(bucket_name, object_key, local_file_path)
            print(f"Downloaded to {local_file_path}")
            return local_file_path

    # If we reach this point, no MP4 file was found.
    print(f"Problem: No MP4 file was found in S3 at {bucket_name}/{invocation_id}")

def t2v_reel(video_prompt:str, model_id:str, role_arn:str, v_length:int, region: str='us-east-1', s3_destination_bucket:str="mmrag-images", region_name:str='us-east-1'):
    """
    Assume an IAM role and create an S3 bucket

    :param role_arn: ARN of the IAM role to assume
    :param bucket_name: Name of the S3 bucket to create
    :param region: AWS region to create the bucket in
    """
    # Configure the inference parameters.
    model_input = {
        "taskType": "TEXT_VIDEO",  # This is the only task type supported in Beta. Other tasks types will be supported at launch
        "textToVideoParams": {"text": video_prompt},
        "videoGenerationConfig": {
            "durationSeconds": v_length,  # 6 is the only supported value currently.
            "fps": 24,  # 24 is the only supported value currently.
            "dimension": "1280x720",  # "1280x720" is the only supported value currently.
            "seed": randint(
                -2147483648, 2147483648
            ),  # A random seed guarantees we'll get a different result each time this code runs.
        },
    }
    #bedrock_runtime = get_boto_client(service_name='bedrock-runtime', region=region_name)
    invocation_jobs = bedrock_runtime.start_async_invoke_model(
        modelId=model_id,
        modelInput=model_input,
        outputDataConfig={"s3OutputDataConfig": {"s3Uri": f"s3://{s3_destination_bucket}"}},
    )
    

    # Check the status of the job until it's complete.
    invocation_arn = invocation_jobs["invocationArn"]
    file_path = ''
    while True:
        invocation_job = bedrock_runtime.get_async_invoke_model(
            invocationArn=invocation_arn
        )
    
        status = invocation_job["status"]
        if status == "InProgress":
            time.sleep(1)
        elif status == "Completed":
            print("\nJob complete!")
            # Save the video to disk.
            s3_bucket = (
                invocation_job["outputDataConfig"]["s3OutputDataConfig"]["s3Uri"]
                .split("//")[1]
                .split("/")[0]
            )
            file_path = download_video_for_invocation_arn(invocation_arn, role_arn, s3_destination_bucket, "./output")
            break
        else:
            print("\nJob failed!")
            print("\nResponse:")
            print(json.dumps(invocation_job, indent=2, default=str))
    return file_path


###
# Luma Ray v2
###
def t2v_luma(video_prompt:str, model_id:str, role_arn:str, v_length:int, region: str='us-west-2', s3_destination_bucket:str="mmrag-images"):
    body = {
        "modelId": model_id,
        "contentType": "application/json",
        "accept": "application/json",
        "prompt": video_prompt,
        "modelInput": {
            #"prompt": video_prompt,
            "aspect_ratio": "16:9",
            "loop": True,
            "duration": v_length,
            "resolution": "1024p"
        },
    }
    #bedrock_runtime = get_boto_client(service_name='bedrock-runtime', region=region_name)
    bedrock_runtime = boto3.client(
        "bedrock-runtime",
        region_name=region,  # You must use us-east-1 during the beta period.
        endpoint_url=f"https://bedrock-runtime.{region}.amazonaws.com",
        config=Config(read_timeout=350) 
    )
    invocation_jobs = bedrock_runtime.start_async_invoke_model(
        modelId=model_id,
        #contentType="application/json",
        #accept="application/json",
        modelInput=body,
        outputDataConfig={"s3OutputDataConfig": {"s3Uri": f"s3://{s3_destination_bucket}"}},
    )
    

    # Check the status of the job until it's complete.
    invocation_arn = invocation_jobs["invocationArn"]
    file_path = ''
    while True:
        invocation_job = bedrock_runtime.get_async_invoke_model(
            invocationArn=invocation_arn
        )
    
        status = invocation_job["status"]
        if status == "InProgress":
            time.sleep(1)
        elif status == "Completed":
            print("\nJob complete!")
            # Save the video to disk.
            s3_bucket = (
                invocation_job["outputDataConfig"]["s3OutputDataConfig"]["s3Uri"]
                .split("//")[1]
                .split("/")[0]
            )
            file_path = download_video_for_invocation_arn(invocation_arn, role_arn, s3_destination_bucket, "./output")
            break
        else:
            print("\nJob failed!")
            print("\nResponse:")
            print(json.dumps(invocation_job, indent=2, default=str))
    return file_path
    
##
# Text
##
def nova_textGen(model_id, prompt, max_tokens, temperature, top_p, top_k, role_arn, region_name='us-east-1'):
    
    request_body = {
        "schemaVersion": "messages-v1",
        "system": [
            {"text": "You are a smart AI assistant whi can answer user questions by providing clear, factual, and dependable information."}
        ],
        "messages": [
            {
                "role": "user",
                "content": [{"text": prompt}]
            },
        ],
        "inferenceConfig": {
            "max_new_tokens": max_tokens,
            "top_p": top_p,
            "top_k": top_k,
            "temperature": temperature,
        }
    }
    messages = [
     {"role": "user", "content": [{"text": prompt}]},
    ]

    # Invoke using invoke
    bedrock_runtime_cris = get_boto_client(service_name='bedrock-runtime', region=region_name)
    response = bedrock_runtime_cris.invoke_model(
        modelId=model_id, #"amazon.Olympus-micro-v1:0",
        body=json.dumps(request_body)
    )
    '''
    # Invoke the model and extract the response body using converse.
    response = bedrock_runtime.converse(
        modelId=model_id, #"amazon.Olympus-micro-v1:0",
        messages=messages
        #body=json.dumps(request_body)
    )
    #Lanhcin
    llm = ChatBedrockConverse(model=model_id)
    messages = [
     ("user", prompt)
    ]
    response = llm.invoke(messages)
    '''
    model_response = json.loads(response["body"].read())
    return model_response["output"]["message"]["content"][0]["text"]

## CRIS
def bedrock_textGen_cris(model_id, prompt, max_tokens, temperature, top_p, top_k, region_name):
    bedrock_runtime_cris = get_boto_client(service_name='bedrock-runtime', region=region_name)
    # Embed the message in Llama 3's prompt format.
    meta_prompt = f"""
    <|begin_of_text|>
    <|start_header_id|>user<|end_header_id|>
    {prompt}
    <|eot_id|>
    <|start_header_id|>assistant<|end_header_id|>
    """
    
    # Format the request payload using the model's native structure.
    request = {
        "prompt": meta_prompt,
        # Optional inference parameters:
        "max_gen_len": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
    }

    # Encode and send the request.
    response = bedrock_runtime_cris.invoke_model(body=json.dumps(request), modelId=model_id)
    
    # Decode the native response body.
    model_response = json.loads(response["body"].read())
    
    # Extract and print the generated text.
    return model_response["generation"]

## Streaming
def nova_textGen_streaming(model_id, prompt, max_tokens, temperature, top_p, top_k, role_arn, region_name='us-east-1'):
    
    request_body = {
        "schemaVersion": "messages-v1",
        "system": [
            {"text": "You are a smart AI assistant whi can answer user questions by providing clear, factual, and dependable information."}
        ],
        "messages": [
            {
                "role": "user",
                "content": [{"text": prompt}]
            },
        ],
        "inferenceConfig": {
            "max_new_tokens": max_tokens,
            "top_p": top_p,
            "top_k": top_k,
            "temperature": temperature,
        }
    }
    start_time = datetime.datetime.now()
    # Invoke the model and extract the response body.
    response = bedrock_runtime.invoke_model_with_response_stream(
        modelId=model_id, #"amazon.Olympus-micro-v1:0",
        body=json.dumps(request_body)
    )
    
    request_id = response.get("ResponseMetadata").get("RequestId")
    chunk_count = 0
    stream_str = ''
    time_to_first_token = None
    # Process the response stream
    stream = response.get("body")
    if stream:
        for event in stream:
            chunk = event.get("chunk")
            if chunk:
                # Print the response chunk
                chunk_json = json.loads(chunk.get("bytes").decode())
                # Pretty print JSON
                # print(json.dumps(chunk_json, indent=2, ensure_ascii=False))
                content_block_delta = chunk_json.get("contentBlockDelta")
                if content_block_delta:
                    if time_to_first_token is None:
                        time_to_first_token = datetime.datetime.now() - start_time
                        print(f"Time to first token: {time_to_first_token}")
    
                    chunk_count += 1
                    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S:%f")
                    # print(f"{current_time} - ", end="")
                    #stream_str = content_block_delta.get("delta").get("text")
                    yield  content_block_delta.get("delta").get("text")
        #print(f"Total chunks: {chunk_count}")
        #return stream_str
    else:
        yield "No response stream received."

    yield f"\n\n ✒︎***Content created by using:*** {model_id}, latency: {str((datetime.datetime.now() - start_time)).replace('0:00:', '')} sec" # * 1000:.2f} ms"
    #model_response = json.loads(response["body"].read())
    #return model_response["output"]["message"]["content"][0]["text"]

###
# Bedrock Claude streaming
###
def bedrock_textGen_streaming(model_id, prompt, max_tokens, temperature, top_p, top_k, region_name):
    # Start a conversation with the user message.
    start_time = datetime.datetime.now()
    conversation = [
        {
            "role": "user",
            "content": [{"text": prompt}],
        }
    ]
    
    try:
        # Send the message to the model, using a basic inference configuration.
        response = bedrock_runtime.converse_stream(
            modelId=model_id,
            messages=conversation,
            inferenceConfig={"maxTokens": max_tokens, "temperature": temperature, "topP": top_p},
        )


        # Extract and print the streamed response text in real-time.
        for chunk in response["stream"]:
            if "contentBlockDelta" in chunk:
                text = chunk["contentBlockDelta"]["delta"]["text"]
                yield f"{text}"
                
        '''
        chunk_count = 0
        stream_str = ''
        time_to_first_token = None
        # Process the response stream
        stream = response.get("body")
        if stream:
            for event in stream:
                chunk = event.get("chunk")
                if chunk:
                    # Print the response chunk
                    chunk_json = json.loads(chunk.get("bytes").decode())
                    # Pretty print JSON
                    # print(json.dumps(chunk_json, indent=2, ensure_ascii=False))
                    content_block_delta = chunk_json.get("contentBlockDelta")
                    if content_block_delta:
                        if time_to_first_token is None:
                            time_to_first_token = datetime.datetime.now() - start_time
                            print(f"Time to first token: {time_to_first_token}")
        
                        chunk_count += 1
                        current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S:%f")
                        # print(f"{current_time} - ", end="")
                        #stream_str = content_block_delta.get("delta").get("text")
                        yield  content_block_delta.get("delta").get("text")
        '''
    except (ClientError, Exception) as e:
        yield f"ERROR: Can't invoke '{model_id}'. Reason: {e}"
        exit(1)

    yield f"\n\n ✒︎***Content created by using:*** {model_id}, latency: {str((datetime.datetime.now() - start_time)).replace('0:00:', '')} sec" # * 1000:.2f} ms"

###
# O1 image 
###
def nova_image(option, prompt, image_binary_data, max_token, temperature, top_p, top_k, role_arn, region="us-east-1"):
    
    #Define your system prompt(s).
    system_list = [
        {
            "type": "system",
            "content": [{"text": "You are a skilled professional in interpreting images. When a user shares an image with you, meticulously examine the visual content and respond to the user's query truthfully and accurately, ensuring all relevant details are included. Take your time to provide a thoughtful and comprehensive answer."}],
        }
    ]
    
    # Define a "user" message including both the image and a text prompt.
    content = [
        {
            "text": prompt
        }
    ]

    for image_bin in image_binary_data:
        base_64_encoded_data = base64.b64encode(image_bin)
        image_base64_string = base_64_encoded_data.decode("utf-8")
        image_type = imghdr.what(io.BytesIO(image_bin))

        content.append(
            {
                "image": {
                    "format": image_type, #"png",
                    "source": {"bytes": image_base64_string},
                }
            }
        )
    
    message_list = [
        {
            "role": "user",
            "content": content
        }
    ]
    
    # Configure the inference parameters.
    inf_params = {"max_new_tokens": max_token, "top_p": top_p, "top_k": top_k, "temperature": temperature}
    
    native_request = {
        "schemaVersion": "messages-v1",
        "messages": message_list,
        "system": system_list,
        "inferenceConfig": inf_params,
    }
    
    # Invoke the model and extract the response body.
    response = bedrock_runtime.invoke_model(modelId=option, body=json.dumps(native_request))
    model_response = json.loads(response["body"].read())

    
    # Return the text content for easy readability.
    return model_response["output"]["message"]["content"][0]["text"]

def o1_video(option, prompt, video_file_name, max_token, temperature, top_p, top_k, role_arn, region="us-east-1"):
    
    #Define your system prompt(s).
    system_list = [
        {
            "type": "system",
            "content": [{"text": "You are an expert video interpreter. Please analyze the provided video  and generate a comprehensive description, including main subjects and their actions, Key events and their temporal sequence, Setting and environmental context and Notable objects and their interactions. Make sure to inlcude Any text or graphical overlays, Audio elements including speech, music, and sound effects, camera movements and transitions, emotional tone and atmosphere, relevant temporal markers or timestamps and any significant changes in scene composition."}],
        }
    ]
    
    # Define a "user" message including both the image and a text prompt. 
    # Value at 'body' failed to satisfy constraint: Member must have length less than or equal to 25000000
    with open(video_file_name, "rb") as video_file:
        video_binary_data = video_file.read()
        base_64_encoded_data = base64.b64encode(video_binary_data) #.getvalue())
        video_base64_string = base_64_encoded_data.decode("utf-8")
        
    message_list = [
        {
            "role": "user",
            "content": [
                {
                    "video": {
                        "format": "mp4",
                        "source": {"bytes": video_base64_string},
                    }
                },
                {
                    "text": prompt
                }
            ],
        }
    ]
    
    # Configure the inference parameters.
    inf_params = {"max_new_tokens": max_token, "top_p": top_p, "top_k": top_k, "temperature": temperature}
    
    native_request = {
        "schemaVersion": "messages-v1",
        "messages": message_list,
        "system": system_list,
        "inferenceConfig": inf_params,
    }
    
    # Invoke the model and extract the response body.
    response = bedrock_runtime.invoke_model(modelId=option, body=json.dumps(native_request))
    model_response = json.loads(response["body"].read())

    
    # Return the text content for easy readability.
    return model_response["output"]["message"]["content"][0]["text"], len(video_binary_data)
    
###
# Search
###
def retrieval_o1(query, documents, model_id, embedding_model_id, chunk_size, over_lap, max_tokens, temperature, top_p, top_k, doc_num, role_arn, region):
    doc2text = "\n\n".join([doc.page_content for doc in documents])
    
    prompt2 = f"{query}. Your answer should be strictly based on the context in {doc2text}."
    return nova_textGen(model_id, prompt2, max_tokens, temperature, top_p, top_k, role_arn, region)
    
def retrieval_faiss_o1(query, documents, model_id, embedding_model_id, chunk_size, over_lap, max_tokens, temperature, top_p, top_k, doc_num, role_arn, region):
    
    request_body = {
        "schemaVersion": "messages-v1",
        "system": [
            {"text": "You are a smart AI assistant whi can answer user questions by providing clear, factual, and dependable information."}
        ],
        "messages": [
            {
                "role": "user",
                "content": [{"text": query}]
            },
        ],
        "inferenceConfig": {
            "max_new_tokens": max_tokens,
            "top_p": top_p,
            "top_k": top_k,
            "temperature": temperature,
        }
    }
    
    # Invoke the model and extract the response body.
    chat = bedrock_runtime.invoke_model(
        modelId=model_id, #"amazon.Olympus-micro-v1:0",
        body=json.dumps(request_body)
    )
    
    # Process doc
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=over_lap, length_function=len, is_separator_regex=False,)
    docs = text_splitter.split_documents(documents)
    
    # Prepare embedding function
    embedding_bedrock = BedrockEmbeddings(client=boto3.client('bedrock-runtime'), model_id=embedding_model_id)
    
    # Try to get vectordb with FAISS
    db = FAISS.from_documents(docs, embedding_bedrock)
    retriever = db.as_retriever(search_kwargs={"k": doc_num})


    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    messages = [
        ("system", """Your are a helpful assistant to provide comprehensive and truthful answers to questions, \n
                    drawing upon all relevant information contained within the specified in {context}. \n 
                    You add value by analyzing the situation and offering insights to enrich your answer. \n
                    Simply say I don't know if you can not find any evidence to match the question. \n
                    """),
        #MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
    
    prompt_template = ChatPromptTemplate.from_messages(messages)

    # Reranker
    compression_retriever = ContextualCompressionRetriever(
        base_compressor= FlashrankRerank(), base_retriever=retriever
    )

    rag_chain = (
        #{"context": compression_retriever | format_docs, "question": RunnablePassthrough()}
        #| RunnableParallel(answer=hub.pull("rlm/rag-prompt") | chat |format_docs, question=itemgetter("question") ) 
        RunnableParallel(context=compression_retriever | format_docs, question=RunnablePassthrough() )
        | prompt_template
        | chat
        | StrOutputParser()
    )

    results = rag_chain.invoke(query)
    return results

### 
#Extract_url using Plympus
###
def extract_urls_o1(urls: list, query: str, model_id: str, embedding_model_id: str,  max_tokens: int, temperature: float, top_p: float, top_k: int, role_arn: str, region: str):
    
    loaders = []
    # Prepare embedding function
    br_embedding = BedrockEmbeddings(client=boto3.client('bedrock-runtime'), model_id=embedding_model_id)

    # Load
    xml_loader = WebBaseLoader(urls[0])
    xml_loader.requests_per_second = 1
    loaders.append(xml_loader)
    if check_urls(urls, '.html') or check_urls(urls, '.htm'):
        html_loader = AsyncHtmlLoader(urls)
        loaders.append(html_loader)
    if check_urls(urls, '.pdf'):
        pdf_loader = PyMuPDFLoader(urls[0])
        loaders.append(pdf_loader)
    loader_all = MergedDataLoader(loaders=loaders)
    docs_all = loader_all.load()

    html2text = Html2TextTransformer()
    docs_transformed = html2text.transform_documents(docs_all)
    text_str = ''
    for doc_transformed in docs_transformed:
        text_str += doc_transformed.page_content

    ''' # Olympus has NOT been tested with LangChain
    docs_all_transformed = html2text.transform_documents(docs_all)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=8000,
        chunk_overlap=800,
        length_function=len,
    )
    
    splits = text_splitter.split_documents(docs_all_transformed)
    # Create a FAISS vector store and embed the document chunks
    vectorstore = FAISS.from_documents(splits, br_embedding)

    retriever = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 1})
    results = retriever.invoke(query, filter={"source": "news"})

    # Create chain
    combine_docs_chain = create_stuff_documents_chain(
           llm_c3, retrieval_qa_chat_prompt
       )
    # Retrivals
    retrieval_chain = create_retrieval_chain(
       vectorstore.as_retriever(), combine_docs_chain
    )
    res = retrieval_chain.invoke({"input":query})
    '''
    prompt2 = f"{query}. Your answer should be strictly based on the context in {text_str}."
    return nova_textGen(model_id, prompt2, max_tokens, temperature, top_p, top_k, role_arn, region)

###
# Local OpenAI for DeepSeek
###
def local_openai_textGen(model_id: str, prompt: str, max_token: int, temperature: float, top_p: float, top_k: int):
    openai_api_key = "EMPTY"
    openai_api_base = "http://agent.cavatar.info:8081/v1"
    
    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )

    chat_response = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role": "system", "content": "You are a helpful assistant. Please answer the user question accurately and truthfully. Also please make sure to think carefully before answering"},
            {"role": "user", "content": prompt},
        ],
        stream=False,
        temperature=temperature,
        max_tokens=max_token,
        top_p=top_p
    )
    usages = f'\n\nCompletion_tokens: {chat_response.usage.completion_tokens}, Prompt_tokens: {chat_response.usage.prompt_tokens},  Total_tokens:{chat_response.usage.total_tokens}'
    return chat_response.choices[0].message.content, usages

def local_gemma3n_textGen(model_id: str, prompt: str, max_token: int, temperature: float, top_p: float, top_k: int):
    openai_api_key = "EMPTY"
    openai_api_base = "http://video.cavatar.info:8087/v1"
    
    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )

    chat_response = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role": "system", "content": "You are a helpful assistant. Please answer the user question accurately and truthfully. Also please make sure to think carefully before answering"},
            {"role": "user", "content": prompt},
        ],
        stream=False,
        temperature=temperature,
        max_tokens=max_token,
        top_p=top_p
    )
    usages = f'\n\nCompletion_tokens: {chat_response.usage.completion_tokens}, Prompt_tokens: {chat_response.usage.prompt_tokens},  Total_tokens:{chat_response.usage.total_tokens}'
    return chat_response.choices[0].message.content, usages

def local_gemma3n_image(message_content, max_token: int):
    openai_api_key = "EMPTY"
    openai_api_base = "http://mcp1.cavatar.info:8081/v1" #"http://video.cavatar.info:8083/v1"
    model_id = "alfredcs/torchrun-medgemma-27b-grpo-merged"
    
    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )

    chat_response = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role": "system", "content": "You are a helpful assistant. Please answer the user question accurately and truthfully."},
            {"role": "user", "content": message_content},
        ],
        max_tokens=max_token,
    )
    return chat_response
    
def local_openai_textGen_streaming(model_id: str, prompt: str, max_token: int, temperature: float, top_p: float, top_k: int):
    openai_api_key = "EMPTY"
    openai_api_base = "http://agent.cavatar.info:8081/v1"
    #openai_api_base = "http://agent.cavatar.info:8081/v1"
    start_time = datetime.datetime.now()
    
    client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )

    chat_response = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role": "system", "content": "You are a helpful assistant. Please answer the user question accurately and truthfully. Also please make sure to think carefully before answering"},
            {"role": "user", "content": prompt},
        ],
        stream=True,
        temperature=temperature,
        max_tokens=max_token,
        top_p=top_p
    )
    for chunk in chat_response:
        yield chunk.choices[0].delta.content #, end="", flush=True)

    #yield f"\n\n ✒︎***Content created by using:*** {model_id}, latency: {str((datetime.datetime.now() - start_time)).replace('0:00:', '')} sec" # * 1000:.2f} ms"
    
###
# Main
###
if __name__ == "__main__":
    prompt = "A high res, 4k image of fall foliage with tree on different layers ofcolors from red, orange, yellow to all season green with snow covered mountain peaks in the backgroung and running creak at the front vivid color and photoreastic"
    neg_prompt = "human, single color tree"
    images = t2i_olympus(prompt, neg_prompt, num_image=3)
    print(f"Image size: {len(images)}")
    video_prompt = "Closeup of a scence of fall foliage in Sierra with snow covered mountain peaks and running stream, frothy gentle wind blowing tree leaves. Camera zoom in."
    file_name = t2v_ovg(video_prompt=video_prompt, role_arn="arn:aws:iam::905418197933:role/ovg_developer")
    print(f"Video file: {file_name}")
